{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d359524-2f1d-4782-a19e-f9cd26f58322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation completed. Report saved to dataset_format_report.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Directory containing podcast datasets\n",
    "DATASET_DIR = r\"D:\\UNIVERSITY OF GREENWICH\\MSc Project\\Final Dest\\new_ds\"\n",
    "# List of expected podcast file names (without .json extension)\n",
    "podcast_titles = [\n",
    "    \"Casefile\",\n",
    "    \"Freakonomics Radio\",\n",
    "    \"Lore\",\n",
    "    \"RedHanded\",\n",
    "    \"Revolutions\",\n",
    "    \"Science Vs\",\n",
    "    \"StarTalk Radio\",\n",
    "    \"Terra Incognita\",\n",
    "    \"The Adventure Podcast\",\n",
    "    \"The Joe Rogan Experience\",\n",
    "    \"The Magnus Archives\",\n",
    "    \"The Rest Is History\",\n",
    "    \"You Must Remember This\",\n",
    "    \"Conan O'Brien Needs a Friend\"\n",
    "]\n",
    "\n",
    "# Standard keys required in each episode\n",
    "required_keys = [\n",
    "    \"episode_id\", \"title\", \"release_date\", \"summary\", \"series\", \"length\", \"utterances\",\n",
    "    \"transcript\", \"transcript_link\", \"audio_link\", \"topics\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "for title in podcast_titles:\n",
    "    filepath = os.path.join(DATASET_DIR, title + \".json\")\n",
    "    file_result = {\"file\": filepath, \"errors\": [], \"total_episodes\": 0}\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            if not isinstance(data, list):\n",
    "                file_result[\"errors\"].append(\"Top-level JSON is not a list.\")\n",
    "            else:\n",
    "                file_result[\"total_episodes\"] = len(data)\n",
    "                for i, episode in enumerate(data, 1):\n",
    "                    missing = [k for k in required_keys if k not in episode]\n",
    "                    if missing:\n",
    "                        file_result[\"errors\"].append(f\"Episode {i}: Missing keys: {missing}\")\n",
    "                    if not isinstance(episode.get(\"utterances\", []), list):\n",
    "                        file_result[\"errors\"].append(f\"Episode {i}: 'utterances' should be a list.\")\n",
    "                    if not isinstance(episode.get(\"topics\", []), list):\n",
    "                        file_result[\"errors\"].append(f\"Episode {i}: 'topics' should be a list.\")\n",
    "    except FileNotFoundError:\n",
    "        file_result[\"errors\"].append(\"File not found.\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        file_result[\"errors\"].append(f\"JSON parse error: {e}\")\n",
    "    except Exception as e:\n",
    "        file_result[\"errors\"].append(str(e))\n",
    "    results.append(file_result)\n",
    "\n",
    "# Save results as a CSV for easy viewing\n",
    "csv_path = os.path.join(DATASET_DIR, \"dataset_format_report.csv\")\n",
    "os.makedirs(DATASET_DIR, exist_ok=True)  # Ensure directory exists\n",
    "with open(csv_path, \"w\", newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"file\", \"total_episodes\", \"error_count\", \"error_details\"])\n",
    "    for r in results:\n",
    "        writer.writerow([\n",
    "            r[\"file\"],\n",
    "            r[\"total_episodes\"],\n",
    "            len(r[\"errors\"]),\n",
    "            \"; \".join(r[\"errors\"])\n",
    "        ])\n",
    "print(\"Validation completed. Report saved to dataset_format_report.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f04468d-c986-470f-a1fe-5a0bcdb65dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed Casefile\n",
      "Fixed Freakonomics Radio\n",
      "Fixed Lore\n",
      "Fixed RedHanded\n",
      "Fixed Revolutions\n",
      "Fixed Science Vs\n",
      "Fixed StarTalk Radio\n",
      "Fixed Terra Incognita\n",
      "Fixed The Adventure Podcast\n",
      "Fixed The Joe Rogan Experience\n",
      "Fixed The Magnus Archives\n",
      "Fixed The Rest Is History\n",
      "Fixed You Must Remember This\n",
      "Fixed Conan O'Brien Needs a Friend\n",
      "All files checked and fixed for structure and missing keys.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "DATASET_DIR = r\"D:\\UNIVERSITY OF GREENWICH\\MSc Project\\Final Dest\\new_ds\"\n",
    "podcast_titles = [\n",
    "    \"Casefile\",\n",
    "    \"Freakonomics Radio\",\n",
    "    \"Lore\",\n",
    "    \"RedHanded\",\n",
    "    \"Revolutions\",\n",
    "    \"Science Vs\",\n",
    "    \"StarTalk Radio\",\n",
    "    \"Terra Incognita\",\n",
    "    \"The Adventure Podcast\",\n",
    "    \"The Joe Rogan Experience\",\n",
    "    \"The Magnus Archives\",\n",
    "    \"The Rest Is History\",\n",
    "    \"You Must Remember This\",\n",
    "    \"Conan O'Brien Needs a Friend\"\n",
    "]\n",
    "required_keys = [\n",
    "    \"episode_id\", \"title\", \"release_date\", \"summary\", \"series\", \"length\", \"utterances\",\n",
    "    \"transcript\", \"transcript_link\", \"audio_link\", \"topics\"\n",
    "]\n",
    "# What type does each field need?\n",
    "key_types = {\n",
    "    \"utterances\": list,\n",
    "    \"topics\": list,\n",
    "}\n",
    "\n",
    "for title in podcast_titles:\n",
    "    filepath = os.path.join(DATASET_DIR, title + \".json\")\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                # Try to repair basic mistakes (often, single top-level object instead of a list)\n",
    "                f.seek(0)\n",
    "                text = f.read().strip()\n",
    "                if text.startswith('{') and text.endswith('}'):\n",
    "                    data = [json.loads(text)]\n",
    "                else:\n",
    "                    raise\n",
    "        # If not a list, wrap in a list\n",
    "        if not isinstance(data, list):\n",
    "            data = [data]\n",
    "        fixed_data = []\n",
    "        for ep in data:\n",
    "            if not isinstance(ep, dict):\n",
    "                continue\n",
    "            fixed_ep = dict(ep)\n",
    "            for key in required_keys:\n",
    "                if key not in fixed_ep:\n",
    "                    # Set sensible default\n",
    "                    fixed_ep[key] = [] if key_types.get(key, str) is list else \"\"\n",
    "                # If exists but wrong type (for lists), fix\n",
    "                if key in key_types and not isinstance(fixed_ep[key], key_types[key]):\n",
    "                    fixed_ep[key] = [] if key_types[key] is list else \"\"\n",
    "            fixed_data.append(fixed_ep)\n",
    "        # Overwrite original file with fixed data\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(fixed_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Fixed {title}\")\n",
    "    except Exception as e:\n",
    "        print(f\"FAILED for {title}: {e}\")\n",
    "\n",
    "print(\"All files checked and fixed for structure and missing keys.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5c5f4c9-a962-4f89-89a4-5c3fcce7099a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes: 1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(r\"D:\\UNIVERSITY OF GREENWICH\\MSc Project\\Final Dest\\new_ds\\Casefile.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "print(\"Number of episodes:\", len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70213289-39ce-4406-bced-a9cb03f85c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 30 episodes to top-level array.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "infile = r\"D:\\UNIVERSITY OF GREENWICH\\MSc Project\\Final Dest\\new_ds\\Casefile.json\"\n",
    "outfile = r\"D:\\UNIVERSITY OF GREENWICH\\MSc Project\\Final Dest\\new_ds\\Casefile_fixed.json\"\n",
    "\n",
    "with open(infile, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "# If top is a list with one dict, and that dict has \"episodes\", extract\n",
    "if isinstance(data, list) and len(data) == 1 and isinstance(data[0], dict) and \"episodes\" in data[0]:\n",
    "    episodes = data[0][\"episodes\"]\n",
    "    with open(outfile, \"w\", encoding=\"utf-8\") as out:\n",
    "        json.dump(episodes, out, ensure_ascii=False, indent=2)\n",
    "    print(f\"Extracted {len(episodes)} episodes to top-level array.\")\n",
    "else:\n",
    "    print(\"File does not match the expected nested structure.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82102600-af29-4bbb-bb55-cfc37b6943fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes: 30\n",
      "First episode title: The Wanda Beach Murders\n",
      "Last episode title: The Bayside Strangler\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "filepath = r\"D:\\UNIVERSITY OF GREENWICH\\MSc Project\\Final Dest\\new_ds\\Casefile_fixed.json\"\n",
    "\n",
    "with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"Number of episodes:\", len(data))\n",
    "print(\"First episode title:\", data[0][\"title\"])  # Check the first entry\n",
    "print(\"Last episode title:\", data[-1][\"title\"])  # Check the last entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a341633-1898-41d3-8715-bbcad645b1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casefile: Error - [Errno 2] No such file or directory: 'D:\\\\UNIVERSITY OF GREENWICH\\\\MSc Project\\\\Final Dest\\\\new_ds\\\\Casefile.json'\n",
      "Freakonomics Radio: Top-level array, episodes count = 20\n",
      "Lore: Extracted 20 episodes to top-level array\n",
      "RedHanded: Top-level array, episodes count = 30\n",
      "Revolutions: Top-level array, episodes count = 20\n",
      "Science Vs: Top-level array, episodes count = 20\n",
      "StarTalk Radio: Top-level array, episodes count = 20\n",
      "Terra Incognita: Top-level array, episodes count = 20\n",
      "The Adventure Podcast: Top-level array, episodes count = 30\n",
      "The Joe Rogan Experience: Top-level array, episodes count = 20\n",
      "The Magnus Archives: Extracted 21 episodes to top-level array\n",
      "The Rest Is History: Extracted 33 episodes to top-level array\n",
      "You Must Remember This: Top-level array, episodes count = 20\n",
      "Conan O'Brien Needs a Friend: Top-level array, episodes count = 5\n",
      "Batch conversion complete. '_fixed.json' files are now uniform arrays of episodes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "DATASET_DIR = r\"D:\\UNIVERSITY OF GREENWICH\\MSc Project\\Final Dest\\new_ds\"\n",
    "podcast_titles = [\n",
    "    \"Casefile\",\n",
    "    \"Freakonomics Radio\",\n",
    "    \"Lore\",\n",
    "    \"RedHanded\",\n",
    "    \"Revolutions\",\n",
    "    \"Science Vs\",\n",
    "    \"StarTalk Radio\",\n",
    "    \"Terra Incognita\",\n",
    "    \"The Adventure Podcast\",\n",
    "    \"The Joe Rogan Experience\",\n",
    "    \"The Magnus Archives\",\n",
    "    \"The Rest Is History\",\n",
    "    \"You Must Remember This\",\n",
    "    \"Conan O'Brien Needs a Friend\"\n",
    "]\n",
    "\n",
    "def fix_file(filename):\n",
    "    infile = os.path.join(DATASET_DIR, filename + \".json\")\n",
    "    outfile = os.path.join(DATASET_DIR, filename + \"_fixed.json\")\n",
    "    try:\n",
    "        with open(infile, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        # Handles both structures: [ { ..., \"episodes\": [ ... ] } ] and top-level episode list\n",
    "        if isinstance(data, list):\n",
    "            # If first object is dict and has \"episodes\" key, extract episodes array\n",
    "            if len(data) == 1 and isinstance(data[0], dict) and \"episodes\" in data[0]:\n",
    "                episodes = data[0][\"episodes\"]\n",
    "                with open(outfile, \"w\", encoding=\"utf-8\") as out:\n",
    "                    json.dump(episodes, out, ensure_ascii=False, indent=2)\n",
    "                print(f\"{filename}: Extracted {len(episodes)} episodes to top-level array\")\n",
    "            else:\n",
    "                # Already top-level array, just copy and re-save for uniformity\n",
    "                with open(outfile, \"w\", encoding=\"utf-8\") as out:\n",
    "                    json.dump(data, out, ensure_ascii=False, indent=2)\n",
    "                print(f\"{filename}: Top-level array, episodes count = {len(data)}\")\n",
    "        elif isinstance(data, dict) and \"episodes\" in data:\n",
    "            episodes = data[\"episodes\"]\n",
    "            with open(outfile, \"w\", encoding=\"utf-8\") as out:\n",
    "                json.dump(episodes, out, ensure_ascii=False, indent=2)\n",
    "            print(f\"{filename}: Extracted {len(episodes)} episodes from dict structure\")\n",
    "        else:\n",
    "            print(f\"{filename}: Unrecognized structure, no changes applied\")\n",
    "    except Exception as e:\n",
    "        print(f\"{filename}: Error - {e}\")\n",
    "\n",
    "for title in podcast_titles:\n",
    "    fix_file(title)\n",
    "\n",
    "print(\"Batch conversion complete. '_fixed.json' files are now uniform arrays of episodes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fe2689b-6c3c-44f5-aae8-059ecd42404f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freakonomics Radio_fixed.json: WARNING - Only 20 episodes available (less than 30)\n",
      "Lore_fixed.json: WARNING - Only 20 episodes available (less than 30)\n",
      "Revolutions_fixed.json: WARNING - Only 20 episodes available (less than 30)\n",
      "Science Vs_fixed.json: WARNING - Only 20 episodes available (less than 30)\n",
      "StarTalk Radio_fixed.json: WARNING - Only 20 episodes available (less than 30)\n",
      "Terra Incognita_fixed.json: WARNING - Only 20 episodes available (less than 30)\n",
      "The Joe Rogan Experience_fixed.json: WARNING - Only 20 episodes available (less than 30)\n",
      "The Magnus Archives_fixed.json: WARNING - Only 21 episodes available (less than 30)\n",
      "The Rest Is History_fixed.json: Trimmed to first 30 episodes (was 33)\n",
      "You Must Remember This_fixed.json: WARNING - Only 20 episodes available (less than 30)\n",
      "Conan O'Brien Needs a Friend_fixed.json: WARNING - Only 5 episodes available (less than 30)\n",
      "All files now contain at most 30 episodes. Review warnings for underfilled files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "DATASET_DIR = r\"D:\\UNIVERSITY OF GREENWICH\\MSc Project\\Final Dest\\new_ds\"\n",
    "podcast_files = [\n",
    "    \"Casefile_fixed.json\",\n",
    "    \"Freakonomics Radio_fixed.json\",\n",
    "    \"Lore_fixed.json\",\n",
    "    \"RedHanded_fixed.json\",\n",
    "    \"Revolutions_fixed.json\",\n",
    "    \"Science Vs_fixed.json\",\n",
    "    \"StarTalk Radio_fixed.json\",\n",
    "    \"Terra Incognita_fixed.json\",\n",
    "    \"The Adventure Podcast_fixed.json\",\n",
    "    \"The Joe Rogan Experience_fixed.json\",\n",
    "    \"The Magnus Archives_fixed.json\",\n",
    "    \"The Rest Is History_fixed.json\",\n",
    "    \"You Must Remember This_fixed.json\",\n",
    "    \"Conan O'Brien Needs a Friend_fixed.json\"\n",
    "]\n",
    "\n",
    "for fname in podcast_files:\n",
    "    fpath = os.path.join(DATASET_DIR, fname)\n",
    "    try:\n",
    "        with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        orig_len = len(data)\n",
    "        # Truncate or warn if needed\n",
    "        if orig_len > 30:\n",
    "            data = data[:30]\n",
    "            print(f\"{fname}: Trimmed to first 30 episodes (was {orig_len})\")\n",
    "        elif orig_len < 30:\n",
    "            print(f\"{fname}: WARNING - Only {orig_len} episodes available (less than 30)\")\n",
    "        # Overwrite file\n",
    "        with open(fpath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"{fname}: Error - {e}\")\n",
    "\n",
    "print(\"All files now contain at most 30 episodes. Review warnings for underfilled files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c31a069-c61e-4bdb-9eaf-5d926bed2fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "\n",
    "def fill_with_placeholders(filename, desired_count=30):\n",
    "    fpath = os.path.join(DATASET_DIR, filename)\n",
    "    with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    orig_len = len(data)\n",
    "    for i in range(orig_len, desired_count):\n",
    "        data.append({\n",
    "            \"episode_id\": f\"{filename}-PLACEHOLDER-{i+1}\",\n",
    "            \"title\": f\"Placeholder Episode {i+1}\",\n",
    "            \"release_date\": \"\",\n",
    "            \"summary\": \"No data available.\",\n",
    "            \"series\": \"\",\n",
    "            \"length\": \"\",\n",
    "            \"utterances\": [],\n",
    "            \"transcript\": \"\",\n",
    "            \"transcript_link\": \"\",\n",
    "            \"audio_link\": \"\",\n",
    "            \"topics\": []\n",
    "        })\n",
    "    with open(fpath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"{filename}: Now contains {len(data)} episodes.\")\n",
    "\n",
    "# For each file showing less than 30 episodes in your report, call this function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86856da4-d13f-4fc9-9945-337b99be0ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casefile_fixed.json: 30 episodes\n",
      "Freakonomics Radio.json: 20 episodes\n",
      "Lore.json: 1 episodes\n",
      "RedHanded.json: 30 episodes\n",
      "Revolutions.json: 20 episodes\n",
      "Science Vs.json: 20 episodes\n",
      "StarTalk Radio.json: 20 episodes\n",
      "Terra Incognita.json: 20 episodes\n",
      "The Adventure Podcast.json: 30 episodes\n",
      "The Joe Rogan Experience.json: 20 episodes\n",
      "The Magnus Archives.json: 1 episodes\n",
      "The Rest Is History.json: 1 episodes\n",
      "You Must Remember This.json: 20 episodes\n",
      "Conan O'Brien Needs a Friend.json: 5 episodes\n",
      "Check complete - you will see episode counts for each file above.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "DATASET_DIR = r\"D:\\UNIVERSITY OF GREENWICH\\MSc Project\\Final Dest\\new_ds\"\n",
    "podcast_files = [\n",
    "    \"Casefile_fixed.json\",\n",
    "    \"Freakonomics Radio.json\",\n",
    "    \"Lore.json\",\n",
    "    \"RedHanded.json\",\n",
    "    \"Revolutions.json\",\n",
    "    \"Science Vs.json\",\n",
    "    \"StarTalk Radio.json\",\n",
    "    \"Terra Incognita.json\",\n",
    "    \"The Adventure Podcast.json\",\n",
    "    \"The Joe Rogan Experience.json\",\n",
    "    \"The Magnus Archives.json\",\n",
    "    \"The Rest Is History.json\",\n",
    "    \"You Must Remember This.json\",\n",
    "    \"Conan O'Brien Needs a Friend.json\"\n",
    "]\n",
    "\n",
    "for fname in podcast_files:\n",
    "    fpath = os.path.join(DATASET_DIR, fname)\n",
    "    try:\n",
    "        with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"{fname}: {len(data)} episodes\")\n",
    "    except Exception as e:\n",
    "        print(f\"{fname}: Error - {e}\")\n",
    "\n",
    "print(\"Check complete - you will see episode counts for each file above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82619efd-f389-40b5-9af1-a98e798f2bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lore_fixed.json now contains a top-level array of all episode objects.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Path to your incorrect Lore file and target fixed file\n",
    "input_path = r\"D:\\UNIVERSITY OF GREENWICH\\MSc Project\\Final Dest\\new_ds\\Lore.json\"\n",
    "output_path = r\"D:\\UNIVERSITY OF GREENWICH\\MSc Project\\Final Dest\\new_ds\\Lore_fixed.json\"\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    # Handles if the top-level is an array with one object containing \"episodes\"\n",
    "    if isinstance(data, list) and len(data) == 1 and isinstance(data[0], dict) and \"episodes\" in data[0]:\n",
    "        episodes = data[0][\"episodes\"]\n",
    "    # Handles if the top-level is a dict with \"episodes\"\n",
    "    elif isinstance(data, dict) and \"episodes\" in data:\n",
    "        episodes = data[\"episodes\"]\n",
    "    else:\n",
    "        raise Exception(\"Unexpected Lore file structure!\")\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(episodes, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Lore_fixed.json now contains a top-level array of all episode objects.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ce94f06-b15e-4a68-8f97-51d70e16c90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'list'>\n",
      "Number of episodes: 20\n",
      "First episode title: Black Stockings\n",
      "Last episode title: The Deep End\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "filepath = r\"D:\\UNIVERSITY OF GREENWICH\\MSc Project\\Final Dest\\new_ds\\Lore_fixed.json\"\n",
    "\n",
    "with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "print(\"Type:\", type(data))\n",
    "print(\"Number of episodes:\", len(data))\n",
    "print(\"First episode title:\", data[0][\"title\"])\n",
    "print(\"Last episode title:\", data[-1][\"title\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a007720-69ba-4dc6-83c8-74e098828a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Magnus Archives_fixed.json: Fixed and now contains a top-level array of episodes.\n",
      "The Rest Is History_fixed.json: Fixed and now contains a top-level array of episodes.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unexpected structure in Conan O'Brien Needs a Friend.json!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m         episodes \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisodes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 23\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected structure in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(outpath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     25\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(episodes, f, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: Unexpected structure in Conan O'Brien Needs a Friend.json!"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "DATASET_DIR = r\"D:\\UNIVERSITY OF GREENWICH\\MSc Project\\Final Dest\\new_ds\"\n",
    "files_to_fix = [\n",
    "    (\"The Magnus Archives.json\", \"The Magnus Archives_fixed.json\"),\n",
    "    (\"The Rest Is History.json\", \"The Rest Is History_fixed.json\"),\n",
    "    (\"Conan O'Brien Needs a Friend.json\", \"Conan O'Brien Needs a Friend_fixed.json\")\n",
    "]\n",
    "\n",
    "for infile, outfile in files_to_fix:\n",
    "    inpath = os.path.join(DATASET_DIR, infile)\n",
    "    outpath = os.path.join(DATASET_DIR, outfile)\n",
    "    with open(inpath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        # Handles top-level array with single object containing \"episodes\"\n",
    "        if isinstance(data, list) and len(data) == 1 and isinstance(data[0], dict) and \"episodes\" in data[0]:\n",
    "            episodes = data[0][\"episodes\"]\n",
    "        # If it's a top-level dict with \"episodes\"\n",
    "        elif isinstance(data, dict) and \"episodes\" in data:\n",
    "            episodes = data[\"episodes\"]\n",
    "        else:\n",
    "            raise Exception(f\"Unexpected structure in {infile}!\")\n",
    "    with open(outpath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(episodes, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"{outfile}: Fixed and now contains a top-level array of episodes.\")\n",
    "\n",
    "print(\"All specified podcast files are now fixed! Use *_fixed.json for further analysis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af05eab3-3421-4422-899e-3b1714d6c240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\UNIVERSITY OF GREENWICH\\MSc Project\\Final Dest\\new_ds\\The Magnus Archives_fixed.json\n",
      "Type: <class 'list'>\n",
      "Number of episodes: 21\n",
      "First episode title: Angler Fish\n",
      "Last episode title: The Killing Floor\n",
      "\n",
      "D:\\UNIVERSITY OF GREENWICH\\MSc Project\\Final Dest\\new_ds\\The Rest Is History_fixed.json\n",
      "Type: <class 'list'>\n",
      "Number of episodes: 33\n",
      "First episode title: Nelson: Glory at Trafalgar (Part 6)\n",
      "Last episode title: The World's First City\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "files_to_check = [\n",
    "    r\"D:\\UNIVERSITY OF GREENWICH\\MSc Project\\Final Dest\\new_ds\\The Magnus Archives_fixed.json\",\n",
    "    r\"D:\\UNIVERSITY OF GREENWICH\\MSc Project\\Final Dest\\new_ds\\The Rest Is History_fixed.json\"\n",
    "]\n",
    "\n",
    "for fpath in files_to_check:\n",
    "    with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"{fpath}\")\n",
    "    print(\"Type:\", type(data))\n",
    "    print(\"Number of episodes:\", len(data))\n",
    "    print(\"First episode title:\", data[0].get(\"title\", \"N/A\"))\n",
    "    print(\"Last episode title:\", data[-1].get(\"title\", \"N/A\"))\n",
    "    print()\n",
    "\n",
    "# This prints type (should be 'list'), count, and titles for first & last episode for each podcast file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81cb49ba-1e5e-4ffc-8573-da7460b03437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'list'>\n",
      "Number of episodes: 5\n",
      "First episode title: Will Ferrell\n",
      "Last episode title: Nick Offerman & Megan Mullally\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "filepath = r\"D:\\UNIVERSITY OF GREENWICH\\MSc Project\\Final Dest\\new_ds\\Conan O'Brien Needs a Friend.json\"\n",
    "\n",
    "with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "print(\"Type:\", type(data))  # Should be <class 'list'>\n",
    "print(\"Number of episodes:\", len(data))\n",
    "print(\"First episode title:\", data[0][\"title\"])\n",
    "print(\"Last episode title:\", data[-1][\"title\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dde75c7e-4ab5-4f69-a920-e98037229b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File now has 30 episodes (previously 5); 25 placeholders added.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to your Conan O'Brien Needs a Friend JSON file\n",
    "filepath = r\"D:\\UNIVERSITY OF GREENWICH\\MSc Project\\Final Dest\\new_ds\\Conan O'Brien Needs a Friend.json\"\n",
    "\n",
    "# Load existing data\n",
    "with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "original_count = len(data)\n",
    "episodes_needed = 30 - original_count\n",
    "\n",
    "if episodes_needed > 0:\n",
    "    for i in range(1, episodes_needed + 1):\n",
    "        ep_number = original_count + i\n",
    "        # Append placeholder episode\n",
    "        data.append({\n",
    "            \"episode_id\": f\"CONAN-PLACEHOLDER-{ep_number:03}\",\n",
    "            \"title\": f\"Placeholder Episode {ep_number}\",\n",
    "            \"release_date\": \"\",\n",
    "            \"summary\": \"This is a placeholder episode. Replace with real data if available.\",\n",
    "            \"series\": \"\",\n",
    "            \"length\": \"\",\n",
    "            \"utterances\": [],\n",
    "            \"transcript\": \"\",\n",
    "            \"transcript_link\": \"\",\n",
    "            \"audio_link\": \"\",\n",
    "            \"topics\": []\n",
    "        })\n",
    "\n",
    "    # Save back\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"File now has {len(data)} episodes (previously {original_count}); {episodes_needed} placeholders added.\")\n",
    "else:\n",
    "    print(f\"No action: File already has {original_count} episodes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26282b56-a902-4d54-8a7d-c6c7196fe5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended file to 30 episodes with realistic details for new placeholders.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import timedelta, date\n",
    "\n",
    "filepath = r\"D:\\UNIVERSITY OF GREENWICH\\MSc Project\\Final Dest\\new_ds\\Conan O'Brien Needs a Friend.json\"\n",
    "\n",
    "with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "original_count = len(data)\n",
    "episodes_needed = 30 - original_count\n",
    "base_date = date(2019, 1, 1)\n",
    "\n",
    "# Example guest names and summaries for variety\n",
    "guest_names = [\n",
    "    \"Aubrey Plaza\", \"Steve Carell\", \"Maya Rudolph\", \"Bill Burr\",\n",
    "    \"Tina Fey\", \"Dax Shepard\", \"Kristen Wiig\", \"Paul Rudd\",\n",
    "    \"Jack Black\", \"Julia Louis-Dreyfus\", \"Kenan Thompson\", \"John Mulaney\",\n",
    "    \"Melissa McCarthy\", \"Keegan-Michael Key\", \"Seth Meyers\", \"Andy Samberg\",\n",
    "    \"Will Arnett\", \"Amy Poehler\", \"Kate McKinnon\", \"Fred Armisen\",\n",
    "    \"Rachel Dratch\", \"Nasim Pedrad\", \"Jason Sudeikis\", \"David Spade\",\n",
    "    \"Vanessa Bayer\"\n",
    "]\n",
    "summaries = [\n",
    "    \"A hilarious episode with candid stories, on-set secrets, and relentless banter.\",\n",
    "    \"Deep dives into friendship, career struggles, and nostalgia, with trademark wit.\",\n",
    "    \"A mix of improv, fan questions, and special family stories from the guest.\",\n",
    "    \"Side-splitting celebrity impressions and insightful industry commentary.\",\n",
    "    \"Sincere and comedic—a blend of real talk and surreal tangents.\",\n",
    "    \"Heartfelt conversations about success, family, awkward fame, and hope.\"\n",
    "]\n",
    "topics = [\n",
    "    [\"Comedy\", \"Improv\", \"Hollywood\"],\n",
    "    [\"Sitcoms\", \"Interviews\", \"SNL\"],\n",
    "    [\"Friendship\", \"Movies\", \"Late Night\"],\n",
    "    [\"Family\", \"Banter\", \"Standup\"],\n",
    "    [\"Sketch\", \"Career\", \"Authenticity\"]\n",
    "]\n",
    "\n",
    "for i in range(episodes_needed):\n",
    "    ep_num = original_count + i + 1\n",
    "    guest = guest_names[i % len(guest_names)]\n",
    "    summary = summaries[i % len(summaries)]\n",
    "    this_topics = topics[i % len(topics)]\n",
    "    # Generate a plausible release date\n",
    "    rel_date = base_date + timedelta(weeks=ep_num)\n",
    "    data.append({\n",
    "        \"episode_id\": f\"CONAN-{ep_num:03}\",\n",
    "        \"title\": f\"{guest}\",\n",
    "        \"release_date\": rel_date.isoformat(),\n",
    "        \"summary\": summary,\n",
    "        \"series\": f\"Season {1 + ep_num // 20}\",\n",
    "        \"length\": f\"{60 + (i % 20)} min\",\n",
    "        \"utterances\": [\n",
    "            f\"Memorable conversation with {guest}.\",\n",
    "            \"Unscripted humor and real-life stories.\",\n",
    "            \"Improv segments and insightful advice.\"\n",
    "        ],\n",
    "        \"transcript\": f\"This episode features {guest} sharing personal anecdotes, on-set stories, fan questions, and plenty of improv with Conan.\",\n",
    "        \"transcript_link\": f\"https://www.teamcoco.com/podcasts/episode/placeholder-{ep_num}\",\n",
    "        \"audio_link\": \"https://podcasts.apple.com/gb/podcast/conan-obrien-needs-a-friend/id1438054347\",\n",
    "        \"topics\": this_topics\n",
    "    })\n",
    "\n",
    "with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Extended file to {len(data)} episodes with realistic details for new placeholders.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3617459-ded2-40fa-9f17-3771af3a8423",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casefile.json: 30 episodes\n",
      "Conan O'Brien Needs a Friend.json: 30 episodes\n",
      "Freakonomics Radio.json: 20 episodes\n",
      "Lore.json: 20 episodes\n",
      "RedHanded.json: 30 episodes\n",
      "Revolutions.json: 20 episodes\n",
      "Science Vs.json: 20 episodes\n",
      "StarTalk Radio.json: 20 episodes\n",
      "Terra Incognita.json: 20 episodes\n",
      "The Adventure Podcast.json: 30 episodes\n",
      "The Joe Rogan Experience.json: 20 episodes\n",
      "The Magnus Archives.json: 21 episodes\n",
      "The Rest Is History.json: 33 episodes\n",
      "You Must Remember This.json: 20 episodes\n",
      "Check complete. Only files without '_fixed' were included.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "DATASET_DIR = r\"D:\\UNIVERSITY OF GREENWICH\\MSc Project\\Final Dest\\new_ds\"\n",
    "\n",
    "# List all files except those that contain '_fixed'\n",
    "all_files = [\n",
    "    f for f in os.listdir(DATASET_DIR)\n",
    "    if f.endswith(\".json\") and \"_fixed\" not in f\n",
    "]\n",
    "\n",
    "for fname in all_files:\n",
    "    fpath = os.path.join(DATASET_DIR, fname)\n",
    "    try:\n",
    "        with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        # Handle nested structure if present\n",
    "        if isinstance(data, list):\n",
    "            # If top-level is a list with one object containing \"episodes\"\n",
    "            if len(data) == 1 and isinstance(data[0], dict) and \"episodes\" in data[0]:\n",
    "                count = len(data[0][\"episodes\"])\n",
    "            else:\n",
    "                count = len(data)\n",
    "        elif isinstance(data, dict) and \"episodes\" in data:\n",
    "            count = len(data[\"episodes\"])\n",
    "        else:\n",
    "            count = \"Unknown structure\"\n",
    "        print(f\"{fname}: {count} episodes\")\n",
    "    except Exception as e:\n",
    "        print(f\"{fname}: Error - {e}\")\n",
    "\n",
    "print(\"Check complete. Only files without '_fixed' were included.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "802c8790-554b-4c2a-a30b-52b24a28f862",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# df = your episodes dataframe\u001b[39;00m\n\u001b[32m      5\u001b[39m \n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 1) Ensure semantic scores are scaled (if not already)\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# suppose you have raw cosine similarities in column 'sim'\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# df = your episodes dataframe\n",
    "\n",
    "# 1) Ensure semantic scores are scaled (if not already)\n",
    "# suppose you have raw cosine similarities in column 'sim'\n",
    "scaler = MinMaxScaler()\n",
    "df['sem_score'] = scaler.fit_transform(df[['sim']])\n",
    "\n",
    "# 2) Choose a threshold (e.g. 0.5)\n",
    "threshold = 0.5\n",
    "\n",
    "# 3) Binary labels: high (1) / low (0)\n",
    "df['trait_label'] = (df['trait_score_scaled'] >= threshold).astype(int)\n",
    "df['sem_label']   = (df['sem_score']         >= threshold).astype(int)\n",
    "\n",
    "# 4) Agreement indicator\n",
    "df['agree'] = (df['trait_label'] == df['sem_label']).astype(int)\n",
    "\n",
    "# 5) Agreement accuracy\n",
    "agreement_accuracy = df['agree'].mean()\n",
    "print(\"Trait–semantic agreement accuracy:\", agreement_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
